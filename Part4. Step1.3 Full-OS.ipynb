{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87dac6cf",
   "metadata": {},
   "source": [
    "# Comp1-传统组学\n",
    "\n",
    "主要适配于传统组学的建模和刻画。典型的应用场景探究rad_score最最终临床诊断的作用。\n",
    "\n",
    "数据的一般形式为(具体文件,文件夹名可以不同)：\n",
    "1. `images`文件夹，存放研究对象所有的CT、MRI等数据。\n",
    "2. `masks`文件夹, 存放手工（Manuelly）勾画的ROI区域。与images文件夹的文件意义对应。\n",
    "3. `label.txt`文件，每个患者对应的标签，例如肿瘤的良恶性、5年存活状态等。\n",
    "\n",
    "## Onekey步骤\n",
    "\n",
    "1. 数据校验，检查数据格式是否正确。\n",
    "2. 组学特征提取，如果第一步检查数据通过，则提取对应数据的特征。\n",
    "3. 读取标注数据信息。\n",
    "4. 特征与标注数据拼接。形成数据集。\n",
    "5. 查看一些统计信息，检查数据时候存在异常点。\n",
    "6. 正则化，将数据变化到服从 N~(0, 1)。\n",
    "7. 通过相关系数，例如spearman、person等筛选出特征。\n",
    "8. 构建训练集和测试集，这里使用的是随机划分，正常多中心验证，需要大家根据自己的场景构建两份数据。\n",
    "9. 通过Lasso筛选特征，选取其中的非0项作为后续模型的特征。\n",
    "10. 使用机器学习算法，例如LR、SVM、RF等进行任务学习。\n",
    "11. 模型结果可视化，例如AUC、ROC曲线，混淆矩阵等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f47352",
   "metadata": {},
   "source": [
    "### 指定数据\n",
    "\n",
    "此模块有3个需要自己定义的参数\n",
    "\n",
    "1. `mydir`: 数据存放的路径。\n",
    "2. `labelf`: 每个样本的标注信息文件。\n",
    "3. `labels`: 要让AI系统学习的目标，例如肿瘤的良恶性、T-stage等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c779d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "from onekey_algo import OnekeyDS as okds\n",
    "from onekey_algo import get_param_in_cwd\n",
    "\n",
    "os.makedirs('img', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('features', exist_ok=True)\n",
    "\n",
    "# 设置任务Task前缀\n",
    "task_type = 'FullOS_'\n",
    "# 设置数据目录\n",
    "# mydir = r'你自己数据的路径'\n",
    "mydir = get_param_in_cwd('radio_dir')\n",
    "# 对应的标签文件\n",
    "group_info = get_param_in_cwd('dataset_column') or 'group'\n",
    "labelf = get_param_in_cwd('label_file')\n",
    "# 读取标签数据列名\n",
    "labels = [get_param_in_cwd('task_column') or 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e9b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_features = []\n",
    "for subset in get_param_in_cwd('subsets'):\n",
    "    rad_feature = pd.read_csv(f'features/{subset}.csv')\n",
    "    rad_feature['group'] = subset\n",
    "    rad_features.append(rad_feature)\n",
    "rad_features = pd.concat(rad_features, axis=0)\n",
    "rad_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9715d329",
   "metadata": {},
   "source": [
    "## 标注数据\n",
    "\n",
    "数据以csv格式进行存储，这里如果是其他格式，可以使用自定义函数读取出每个样本的结果。\n",
    "\n",
    "要求label_data为一个`DataFrame`格式，包括ID列以及后续的labels列，可以是多列，支持Multi-Task。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76b576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onekey_algo.custom.components.comp1 import fillna\n",
    "\n",
    "event_col = get_param_in_cwd('event_col', 'OS')\n",
    "duration_col= get_param_in_cwd('duration_col', 'OS_time')\n",
    "label_data = pd.read_csv(get_param_in_cwd('survival_file'), dtype={'ID': str})\n",
    "label_data = pd.merge(label_data, pd.read_csv('joinit_group.csv')[['ID']])\n",
    "label_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92814b27",
   "metadata": {},
   "source": [
    "## 特征拼接 \n",
    "\n",
    "将标注数据`label_data`与`rad_data`进行合并，得到训练数据。\n",
    "\n",
    "**注意：** \n",
    "1. 需要删掉ID这一列\n",
    "2. 如果发现数据少了，需要自行检查数据是否匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78982ba4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from onekey_algo.custom.utils import print_join_info\n",
    "\n",
    "print_join_info(rad_features, label_data)\n",
    "combined_data = pd.merge(rad_features, label_data[['ID', event_col, duration_col]], on=['ID'], how='inner')\n",
    "print(combined_data['group'].value_counts())\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e19ce",
   "metadata": {},
   "source": [
    "## 获取到数据的统计信息\n",
    "\n",
    "1. count，统计样本个数。\n",
    "2. mean、std, 对应特征的均值、方差\n",
    "3. min, 25%, 50%, 75%, max，对应特征的最小值，25,50,75分位数，最大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec35ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6fc73b",
   "metadata": {},
   "source": [
    "## 正则化\n",
    "\n",
    "`normalize_df` 为onekey中正则化的API，将数据变化到0均值1方差。正则化的方法为\n",
    "\n",
    "$column = \\frac{column - mean}{std}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be06d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onekey_algo.custom.components.comp1 import normalize_df\n",
    "\n",
    "data = normalize_df(combined_data, not_norm=['ID', event_col, duration_col], group='group', use_train=True)\n",
    "data = data.dropna(axis=1)\n",
    "data.to_csv(f'features/{task_type}feature_norm.csv', index=False)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1662f002",
   "metadata": {},
   "source": [
    "### 相关系数\n",
    "\n",
    "计算相关系数的方法有3种可供选择\n",
    "1. pearson （皮尔逊相关系数）: standard correlation coefficient\n",
    "\n",
    "2. kendall (肯德尔相关性系数) : Kendall Tau correlation coefficient\n",
    "\n",
    "3. spearman (斯皮尔曼相关性系数): Spearman rank correlation\n",
    "\n",
    "三种相关系数参考：https://blog.csdn.net/zmqsdu9001/article/details/82840332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onekey_algo.custom.components.comp1 import normalize_df, select_feature\n",
    "\n",
    "corr_name = get_param_in_cwd('corr_name', 'pearson')\n",
    "if os.path.exists(f'features/{task_type}rad_features_corrsel.csv') and False:\n",
    "    data = pd.read_csv(f'features/{task_type}rad_features_corrsel.csv', header=0)\n",
    "else:\n",
    "    tgroup = data[data['group'] == 'train']\n",
    "    sel_feature = select_feature(tgroup[[c for c in tgroup.columns if c not in [event_col, duration_col]]].corr(corr_name), \n",
    "                                 threshold=0.9, topn=1, verbose=False)\n",
    "    data = data[['ID'] + sel_feature + [event_col, duration_col, 'group']]\n",
    "    data.to_csv(f'features/{task_type}rad_features_corrsel.csv', header=True, index=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4be710",
   "metadata": {},
   "source": [
    "## 构建数据\n",
    "\n",
    "将样本的训练数据X与监督信息y分离出来，并且对训练数据进行划分，一般的划分原则为80%-20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8058ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onekey_algo.custom.components as okcomp\n",
    "from collections import OrderedDict\n",
    "\n",
    "train_data = data[(data[group_info] == 'train')]\n",
    "\n",
    "# subsets = [s for s in label_data['group'].value_counts().index if s != 'train']\n",
    "subsets = get_param_in_cwd('subsets')\n",
    "val_datasets = OrderedDict()\n",
    "for subset in subsets:\n",
    "    val_data = data[data[group_info] == subset]\n",
    "    val_datasets[subset] = val_data\n",
    "    val_data.to_csv(f'features/{task_type}{subset}_features_norm.csv', index=False)\n",
    "\n",
    "print('，'.join([f\"{subset}样本数：{d_.shape}\" for subset, d_ in val_datasets.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9836ff",
   "metadata": {},
   "source": [
    "# 单因素Cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66015b36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from onekey_algo.custom.components.survival import uni_cox\n",
    "\n",
    "if os.path.exists(f'features/{task_type}{duration_col}_rad_features_unisel.csv') and False:\n",
    "    train_data = pd.read_csv(f'features/{task_type}{duration_col}_rad_features_unisel.csv')\n",
    "else:\n",
    "    sel_features = uni_cox(train_data, duration_col=duration_col, event_col=event_col,\n",
    "                           cols=[c for c in train_data.columns if c not in [event_col, duration_col, 'ID', 'group']], \n",
    "                           verbose=False, topk=16)\n",
    "    sel_features = [f for f in sel_features if f not in ['Vimentin', 'HE4', 'CA125']]\n",
    "    train_data = train_data[['ID'] + sel_features + [event_col, duration_col, 'group']]\n",
    "    train_data.to_csv(f'features/{task_type}{duration_col}_rad_features_unisel.csv', header=True, index=False)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af574a87",
   "metadata": {},
   "source": [
    "# Cox-Lasso特征筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece81833",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from onekey_algo.custom.components.survival import get_x_y_survival, lasso_cox_cv\n",
    "COEF_THRESHOLD = 1e-6\n",
    "\n",
    "X, y = get_x_y_survival(train_data, val_outcome=1, event_col=event_col, duration_col=duration_col)\n",
    "sel_features = lasso_cox_cv(X, y, max_iter=100,  norm_X=False, prefix=f\"{task_type}\", l1_ratio=0.1, cv=10, weights_fig_size=(10, 15))\n",
    "# sel_features = lasso_cox_cv(X, y, max_iter=1000,  norm_X=False, prefix=f\"{task}_\", l1_ratio=0.8, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e37576",
   "metadata": {},
   "source": [
    "# 筛选特征\n",
    "\n",
    "使用Lasso-Cox之后的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b092ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[['ID'] + list(sel_features.index) + [event_col, duration_col]]\n",
    "for subset in subsets:\n",
    "    val_datasets[subset] = val_datasets[subset][['ID'] + list(sel_features.index) + [event_col, duration_col]]\n",
    "    val_datasets[subset].to_csv(f'features/{task_type}{subset}_cox.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f2c0ae",
   "metadata": {},
   "source": [
    "# 筛选特征的聚类分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c99f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if train_data.shape[1] < 150:\n",
    "    pp = sns.clustermap(train_data[[c for c in train_data.columns if c not in [event_col, duration_col]]].corr(corr_name), \n",
    "                        linewidths=.5, figsize=(20.0, 16.0), cmap='YlGnBu')\n",
    "    plt.setp(pp.ax_heatmap.get_yticklabels(), rotation=0)\n",
    "    plt.savefig(f'img/{task_type}feature_cluster.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3a9dc",
   "metadata": {},
   "source": [
    "# Cox建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0337a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "\n",
    "cph = CoxPHFitter(penalizer=0.31)\n",
    "cph.fit(train_data[[c for c in train_data.columns if c != 'ID']], duration_col=duration_col, event_col=event_col)\n",
    "cph.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7e49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cph.concordance_index_)\n",
    "su = cph.summary[['exp(coef)', 'exp(coef) lower 95%', 'exp(coef) upper 95%', 'p']]\n",
    "su.columns = ['HR', 'HR lower 95%', 'HR upper 95%', 'pvalue']\n",
    "su.reset_index().to_csv(f'features/{task_type}features_HR.csv', index=False)\n",
    "su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0a36fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "cph.plot(hazard_ratios=True)\n",
    "plt.savefig(f'img/{task_type}feature_pvalue.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onekey_algo.custom.components.ugly import drop_survival\n",
    "\n",
    "# del_num = {'val': 4, 'test': 6, 'test2': 14}\n",
    "# for subset in ['test']:\n",
    "#     val_data = val_datasets[subset]\n",
    "#     cox_data = val_data[[c for c in val_data.columns if c not in ['group']]]\n",
    "#     kid = drop_survival(cox_data, cph, drop_num=del_num[subset], is_drop_ids=False)\n",
    "#     display(val_data[~val_data['ID'].isin(kid['ID'])])\n",
    "#     val_data = pd.merge(val_data, kid[['ID']], on='ID', how='inner')\n",
    "#     val_datasets[subset] = val_data\n",
    "#     val_datasets[subset+'_sel'] = val_data\n",
    "\n",
    "print('，'.join([f\"{subset}样本数：{d_.shape[0]}\" for subset, d_ in val_datasets.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a96c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lifelines import CoxPHFitter\n",
    "# mci = 0\n",
    "# for p in range(1, 100):\n",
    "#     cph = CoxPHFitter(penalizer=p/100)\n",
    "#     cph.fit(train_data[[c for c in train_data.columns if c != 'ID']], duration_col=duration_col, event_col=event_col)\n",
    "#     test_data = val_datasets['test']\n",
    "#     ci = cph.score(test_data[[c for c in test_data.columns if c != 'ID']], scoring_method=\"concordance_index\")\n",
    "#     if mci < ci:\n",
    "#         print(p, ci)\n",
    "#         mci = ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518786c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.plotting import add_at_risk_counts\n",
    "\n",
    "thres = 0.05\n",
    "bst_split = {'train': 1.48, 'val': 0.98, 'test':0.77}\n",
    "for subset, test_data in val_datasets.items():\n",
    "    c_index = cph.score(test_data[[c for c in test_data.columns if c != 'ID']], scoring_method=\"concordance_index\")\n",
    "#     y_pred = cph.predict_median(test_data[[c for c in test_data.columns if c != 'ID']])\n",
    "#     cox_data = pd.concat([test_data, y_pred], axis=1)\n",
    "#     mean = cox_data.describe()[0.5]['mean']\n",
    "#     cox_data['HR'] = cox_data[0.5] < mean\n",
    "    y_pred = cph.predict_partial_hazard(test_data[[c for c in test_data.columns if c != 'ID']])\n",
    "    cox_data = pd.concat([test_data, y_pred], axis=1)\n",
    "    mean = cox_data.describe()[0]['50%']\n",
    "    cox_data['HR'] = cox_data[0] > bst_split[subset]\n",
    "#     cox_data['HR'] = cox_data[0] > 1\n",
    "\n",
    "    dem = (cox_data[\"HR\"] == True)\n",
    "    results = logrank_test(cox_data[duration_col][dem], cox_data[duration_col][~dem], \n",
    "                           event_observed_A=cox_data[event_col][dem], event_observed_B=cox_data[event_col][~dem])\n",
    "    p_value = f\"={results.p_value:.4f}\" if results.p_value > thres else f'<{thres}'\n",
    "    plt.title(f\"Cohort {subset} C-index:{c_index:.3f}, p_value{p_value}\")\n",
    "    if sum(dem):\n",
    "        kmf_high = KaplanMeierFitter()\n",
    "        kmf_high.fit(cox_data[duration_col][dem], event_observed=cox_data[event_col][dem], label=\"High Risk\")\n",
    "        kmf_high.plot_survival_function(color='r')\n",
    "    if sum(~dem):\n",
    "        kmf_low = KaplanMeierFitter()\n",
    "        kmf_low.fit(cox_data[duration_col][~dem], event_observed=cox_data[event_col][~dem], label=\"Low Risk\")\n",
    "        kmf_low.plot_survival_function(color='g')\n",
    "    add_at_risk_counts(kmf_high, kmf_low, rows_to_show=['At risk'])\n",
    "    plt.savefig(f'img/{task_type}KM_{subset}.svg', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e58e09",
   "metadata": {},
   "source": [
    "# 铂敏感预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0601d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.plotting import add_at_risk_counts\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "\n",
    "pt_sens = pd.read_csv('joinit_group.csv')\n",
    "youden = {'train': 0.162, 'val':0.160, 'test': 0.161}\n",
    "pt_sens_preds = []\n",
    "for subset in get_param_in_cwd('subsets'):\n",
    "    pt_sens_pred = pd.read_csv(f'results/Fusion_Transformer_{subset}.csv')\n",
    "    pt_sens_pred['group'] = subset\n",
    "    pt_sens_pred['label'] = (pt_sens_pred['label-1'] > youden[subset]).astype(int)\n",
    "    pt_sens_preds.append(pt_sens_pred)\n",
    "    \n",
    "pt_sens_preds = pd.concat(pt_sens_preds, axis=0)\n",
    "\n",
    "for subset, test_data in val_datasets.items():\n",
    "    cox_data = pd.merge(test_data, pt_sens_preds, on='ID', how='inner')\n",
    "    c_index = concordance_index(cox_data[duration_col], 1-cox_data['label-1'], cox_data[event_col])\n",
    "    cox_data['HR'] = cox_data['label'] > youden[subset]\n",
    "#     mean = cox_data.describe()['label-1']['50%']\n",
    "#     cox_data['HR'] = cox_data['label-1'] > mean\n",
    "    dem = (cox_data[\"HR\"] == True)\n",
    "    results = logrank_test(cox_data[duration_col][dem], cox_data[duration_col][~dem], \n",
    "                           event_observed_A=cox_data[event_col][dem], event_observed_B=cox_data[event_col][~dem])\n",
    "    p_value = f\"={results.p_value:.4f}\" if results.p_value > thres else f'<{thres}'\n",
    "    plt.title(f\"Cohort {subset} C-index:{c_index:.3f}, p_value{p_value}\")\n",
    "    if sum(dem):\n",
    "        kmf_high = KaplanMeierFitter()\n",
    "        kmf_high.fit(cox_data[duration_col][dem], event_observed=cox_data[event_col][dem], label=\"High Risk\")\n",
    "        kmf_high.plot_survival_function(color='r')\n",
    "    if sum(~dem):\n",
    "        kmf_low = KaplanMeierFitter()\n",
    "        kmf_low.fit(cox_data[duration_col][~dem], event_observed=cox_data[event_col][~dem], label=\"Low Risk\")\n",
    "        kmf_low.plot_survival_function(color='g')\n",
    "    add_at_risk_counts(kmf_high, kmf_low, rows_to_show=['At risk'])\n",
    "    plt.savefig(f'img/{task_type}_PT_KM_{subset}.svg', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a79f82",
   "metadata": {},
   "source": [
    "# 保存预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca5360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_prediction(model: CoxPHFitter, data, ID=None, **kwargs):\n",
    "    hr = model.predict_partial_hazard(data)\n",
    "    expectation = model.predict_expectation(data)\n",
    "    \n",
    "    predictions = pd.concat([hr, expectation], axis=1)\n",
    "    predictions.columns = ['HR', 'expectation']\n",
    "    if ID is not None:\n",
    "        predictions = pd.concat([ID, hr, expectation], axis=1)\n",
    "        predictions.columns = ['ID', 'HR', 'expectation']\n",
    "    else:\n",
    "        predictions = pd.concat([hr, expectation], axis=1)\n",
    "        predictions.columns = ['HR', 'expectation']\n",
    "    return predictions\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "info = []\n",
    "for subset, test_data in val_datasets.items():\n",
    "    if subset in get_param_in_cwd('subsets'):\n",
    "        results = get_prediction(cph, test_data, ID=test_data['ID'])\n",
    "        results.to_csv(f'results/{task_type}cox_predictions_{subset}.csv', index=False)\n",
    "        results['group'] = subset\n",
    "        info.append(results)\n",
    "        pd.merge(results, label_data[['ID', event_col, duration_col]], on='ID', how='inner').to_csv(f'features/{task_type}4xtile_{subset}.txt', \n",
    "                                                                                                    index=False, sep='\\t')\n",
    "info = pd.concat(info, axis=0)\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4653b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info[['ID', 'group']].to_csv('group.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb02221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e12a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
